\documentclass[
11pt,notheorems,hyperref={pdfauthor=whatever}
]{beamer}

\input{loadslides.tex}

\usepackage{tabularx}
\usepackage{array}
\usepackage{booktabs}


\title[MARRVEL-MCP Revision]{MARRVEL-MCP Revision}

\author[]{Hyun-Hwan Jeong}

\institute{Liu Lab Meeting}
\date{February 4, 2026}

\begin{document}

% Title page
{
\setbeamertemplate{footline}{}
\begin{frame}
  \titlepage
\end{frame}
}
\addtocounter{framenumber}{-1}

%==============================================================================
\section{Reviewer \#1}
%==============================================================================

\begin{frame}{Reviewer \#1: General Comments}

\begin{quote}
\small
This manuscript describes MARRVEL-MCP, an AI-agent system that automatically navigates appropriate biomedical resources to answer rare disease questions. This is a \textbf{very timely and important topic} for Mendelian disease discovery in the era of AI. The reviewer has several suggestions for improving the manuscript.
\end{quote}

\vspace{0.4em}

\textbf{6 specific comments:}
\begin{enumerate}
  \item Related work on biomedical AI agents
  \item Clarification of ``context engineering''
  \item Benchmark dataset and evaluation details
  \item Comparison with commercial AI tools (GPT, Gemini)
  \item Title wording clarification
  \item Usability improvement evidence
\end{enumerate}

\end{frame}

%--- R1.1 ---
\begin{frame}{R1.1: Related Work on Biomedical AI Agents}

\textbf{Reviewer comment:}
\begin{quote}
The authors should consider including more relevant work on biomedical AI agents (e.g., BioMNI). It would also be helpful to compare MARRVEL-MCP with existing biomedical AI agents in terms of architecture, performance, speed, etc., to better highlight the uniqueness of the proposed system for rare diseases.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Add BioMNI and other biomedical AI agent references
  \item Include comparison table: architecture, performance, speed
  \item Emphasize rare disease specialization as differentiator
\end{itemize}

\end{frame}

%--- R1.2 ---
\begin{frame}{R1.2: Clarification of ``Context Engineering''}

\textbf{Reviewer comment:}
\begin{quote}
The term \emph{context engineering} has been used to describe the next generation of RAG systems. Its definition typically extends beyond selecting appropriate input resources, which appears to be the main definition used in this study. Please ensure that the usage in the manuscript is precise and consistent with existing terminology.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Review broader definition of context engineering (RAG 2.0)
  \item Clarify our usage vs. the general definition
  \item Ensure consistent terminology throughout manuscript
\end{itemize}

\end{frame}

%--- R1.3 ---
\begin{frame}{R1.3: Benchmark Dataset and Evaluation Details}

\textbf{Reviewer comment:}
\begin{quote}
Please provide additional details about the benchmark dataset and evaluation pipeline. What criteria were used to define ``answerable'' questions? Were answers manually generated by experts? When Claude Sonnet was used as an external judge, did it compare with expert-generated reference answers, or rely on its own internal knowledge?
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Detail criteria for ``answerable'' questions
  \item Clarify expert answer generation process
  \item Explain Claude Sonnet judge: reference answers vs. internal knowledge
  \item Add evaluation pipeline diagram or flowchart
\end{itemize}

\end{frame}

%--- R1.4 ---
\begin{frame}{R1.4: Comparison with Commercial AI Tools}

\textbf{Reviewer comment:}
\begin{quote}
The authors should include the performance of state-of-the-art commercial AI tools such as GPT and Gemini on the benchmark dataset. If these systems already achieve strong performance, please justify the added value of MARRVEL-MCP (e.g., smaller models, lower cost, domain customization, transparency, or reproducibility).
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Benchmark GPT-4o / GPT-o1 and Gemini on evaluation set
  \item Report performance comparison table
  \item Justify added value: domain tools, cost, transparency, reproducibility
\end{itemize}

\end{frame}

%--- R1.5 ---
\begin{frame}{R1.5: Title Wording Clarification}

\textbf{Reviewer comment:}
\begin{quote}
Is MARRVEL-MCP primarily a question-answering system, or does it function as a chatbot that supports multi-turn conversations? I suggest replacing ``NATURAL-LANGUAGE QUERY-TO-RESPONSE INTERFACE'' in the title with ``Question-Answering System'' or ``Chatbot'' to improve clarity and precision.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Decide: QA system vs. chatbot vs. current wording
  \item Clarify single-turn vs. multi-turn capability in manuscript
  \item Revise title accordingly
\end{itemize}

\end{frame}

%--- R1.6 ---
\begin{frame}{R1.6: Usability Improvement Evidence}

\textbf{Reviewer comment:}
\begin{quote}
The authors have noted that user-friendliness is one of the motivations for developing MARRVEL-MCP. However, is there any evidence showing that MARRVEL-MCP provides outputs that are more beneficial to users? If there has not been a formal user study, the authors should consider addressing this point briefly in the discussion section.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Acknowledge lack of formal user study (if applicable)
  \item Add discussion paragraph on usability benefits
  \item Consider citing indirect evidence or anecdotal feedback
  \item Mention user study as future work
\end{itemize}

\end{frame}

%==============================================================================
\section{Reviewer \#2}
%==============================================================================

\begin{frame}{Reviewer \#2: General Comments}

\begin{quote}
\small
I think this paper is on the right track, but right now it feels more like a \textbf{solid engineering effort} than a big conceptual leap. The MCP tools are genuinely useful, but the manuscript could be more honest about what is new versus what is just well-integrated. The MCP layers the authors built are probably the \textbf{most valuable part} of the work. They make it much easier for LLMs to interact with biological databases like dbNSFP, which is great for the community. The tooling infrastructure is the most compelling contribution, but the \textbf{system-level claims need stronger justification} and clearer evidence.
\end{quote}

\vspace{0.4em}

\textbf{6 specific comments:}
\begin{enumerate}
  \item Context engineering already in most agent systems
  \item ``Hard-coded'' vs. non-hard-coded clarification
  \item Evaluation is thin; hand-picked examples
  \item Which MCP tools help the most?
  \item Error types count mismatch (Sec. 3.3)
  \item Gold standard reliability concern
\end{enumerate}

\end{frame}

%--- R2.1 ---
\begin{frame}{R2.1: Context Engineering in Agent Systems}

\textbf{Reviewer comment:}
\begin{quote}
In most agent systems, context engineering is already part of the architecture, so the comparison in the intro is confusing.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Clarify what is novel about our context engineering approach
  \item Distinguish from standard agent-system context handling
  \item Rewrite intro comparison to avoid confusion
\end{itemize}

\end{frame}

%--- R2.2 ---
\begin{frame}{R2.2: Hard-Coded vs. Non-Hard-Coded}

\textbf{Reviewer comment:}
\begin{quote}
Authors emphasized not ``hard-coded''; I partially agree. But given many coding is implemented in the MCP layer (providing interfaces), I won't say it is completely non hard-coded. The improvement of MCP over the baseline is because of the ``hard-coding'' that exposes the abstract layer usable by LLM agents. A better way to describe: MCP is certainly hard-coded, but the workflow is not.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Adopt nuanced framing: MCP tools are hard-coded interfaces, \\ but the \textbf{workflow/reasoning} is not hard-coded
  \item Revise manuscript language accordingly
\end{itemize}

\end{frame}

%--- R2.3 ---
\begin{frame}{R2.3: Evaluation Concerns}

\textbf{Reviewer comment:}
\begin{quote}
The current evaluation is a bit thin. The examples in Section 3.1 are hand-picked and likely biased. For the 45 questions, it's not clear which ones actually need MCP tools and which ones don't. I'd really like to see cases when questions do not need MCP tools, how it performed comparing to the baseline.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Categorize 45 questions: MCP-required vs. MCP-optional
  \item Report performance stratified by category
  \item Show baseline performance on non-MCP questions
  \item Address selection bias in Section 3.1 examples
\end{itemize}

\end{frame}

%--- R2.4 ---
\begin{frame}{R2.4: Tool Usage Analysis}

\textbf{Reviewer comment:}
\begin{quote}
It would be helpful to know which MCP tools actually help the most? How many tool calls are needed per question? Whether performance drops as tool usage increases?
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Report per-tool contribution to answer quality
  \item Analyze distribution of tool calls per question
  \item Investigate performance vs. number of tool calls
  \item Add tool usage breakdown figure or table
\end{itemize}

\end{frame}

%--- R2.5 ---
\begin{frame}{R2.5: Error Types Count Mismatch}

\textbf{Reviewer comment:}
\begin{quote}
There's a small error where the paper says there are three error types but only lists two. (Section 3.3: ``We categorized errors into three types based on their underlying cause.'')
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Fix: either add the missing third error type or correct the count
  \item Proofread Section 3.3 thoroughly
\end{itemize}

\end{frame}

%--- R2.6 ---
\begin{frame}{R2.6: Gold Standard Reliability}

\textbf{Reviewer comment:}
\begin{quote}
As LLMs become stronger, the reliability of purely human-curated ``gold standard'' answers becomes less clear. It would be helpful to know whether the authors manually reviewed LLM-generated ``error'' answers to confirm they are truly incorrect, rather than alternative valid answers.
\end{quote}

\vspace{0.6em}
\textbf{Action items:}
\begin{itemize}
  \item Manually review LLM answers marked as ``errors''
  \item Report how many ``errors'' were actually valid alternative answers
  \item Discuss limitations of human-curated gold standards
  \item Consider inter-annotator agreement or adjudication process
\end{itemize}

\end{frame}

\end{document}
